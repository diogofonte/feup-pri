{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Information Processing and Retrieval**\n",
    "\n",
    "**Project developed by:**\n",
    "- Diogo Fonte - up202004175\n",
    "- Rodrigo Figueiredo - up202005216\n",
    "- Sofia Rodrigo  - up202301429\n",
    "- VÃ­tor Cavaleiro - up202004724\n",
    "\n",
    "## **Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "### All The News - Collection of Articles from 18 publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original file is a .db file, which was exported as a json file using the sqlite studio\n",
    "\n",
    "# get table with rows and columns\n",
    "f = open(\"../data/all-the-news/all-the-news-conv.json\", encoding=\"utf8\")\n",
    "data = json.load(f)\n",
    "table = data[\"objects\"][0]\n",
    "\n",
    "# get rows and columns\n",
    "columns = table[\"columns\"]\n",
    "rows = table[\"rows\"]\n",
    "\n",
    "# get column names\n",
    "column_names = []\n",
    "for column in columns:\n",
    "    column_names.append(column[\"name\"])\n",
    "\n",
    "# Create resulting dictionary\n",
    "result = {}\n",
    "for column_name in column_names:\n",
    "    result[column_name] = []\n",
    "\n",
    "# get rows\n",
    "for row in rows:\n",
    "    for i in range(len(column_names)):\n",
    "        result[column_names[i]].append(row[i])\n",
    "\n",
    "pd.DataFrame.from_dict(result).to_csv('all_the_news.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_news = pd.read_csv('all_the_news.csv', encoding='utf-8')\n",
    "all_the_news.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops irrelevant columns\n",
    "all_the_news = all_the_news.drop(columns=['Unnamed: 0', 'id', 'year', 'month', 'digital'])\n",
    "all_the_news = all_the_news.rename(columns={\"publication\": \"publisher\"})\n",
    "all_the_news = all_the_news.rename(columns={\"category\": \"source\"})\n",
    "all_the_news = all_the_news.rename(columns={\"section\": \"category\"})\n",
    "all_the_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_count = all_the_news.shape[0]\n",
    "print(\"Number of rows: \", rows_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"../data/bbc_news_collection/\"\n",
    "news = []\n",
    "\n",
    "# Iterate through subfolders of the 5 categories (business, entertainment, politics, sport, tech)\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(subfolder_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                    author = np.nan  # No author information\n",
    "                    date = \"2005-12-31\"\n",
    "                    lines = file.readlines()\n",
    "                    title = lines[0].strip()  # Read the first line as the title\n",
    "                    content = \"\".join(lines[1:]).replace(\"\\n\", \" \").strip()  # Read the rest as content\n",
    "                    publisher = \"BBC\"\n",
    "                    category = subfolder\n",
    "                    url = np.nan\n",
    "                    source = \"website\"\n",
    "\n",
    "                    aux = pd.DataFrame({\"title\": [title], \"author\": [author], \"date\": [date],\n",
    "                                        \"content\": [content], \"publisher\": [publisher],  \"source\": [source],\n",
    "                                        \"category\": [category], \"url\": [url]})\n",
    "                    news.append(aux)\n",
    "\n",
    "bbc_news = pd.concat(news, ignore_index=True)\n",
    "bbc_news.to_csv(\"bbc_articles.csv\", index=False)\n",
    "bbc_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_count_bbc = bbc_news.shape[0]\n",
    "print(\"Number of rows: \", rows_count_bbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.concat([all_the_news, bbc_news], ignore_index=True)\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_count_news_dataset = news_dataset.shape[0]\n",
    "print(\"Number of rows: \", rows_count_news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['title', 'author', 'content', 'publisher', 'source', 'category', 'url']\n",
    "\n",
    "# strip the text columns\n",
    "news_dataset[text_columns] = news_dataset[text_columns].apply(lambda x: x.str.strip())\n",
    "\n",
    "# Remove duplicates\n",
    "news_dataset = news_dataset.drop_duplicates(subset=['title'], keep='first')\n",
    "news_dataset = news_dataset.drop_duplicates(subset=['content'], keep='first')\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "news_dataset[text_columns] = news_dataset[text_columns].replace('', np.nan)\n",
    "\n",
    "# drop na values from title and content columns\n",
    "news_dataset = news_dataset.dropna(subset=['title'])\n",
    "news_dataset = news_dataset.dropna(subset=['content'])\n",
    "\n",
    "# Convert 'date' column to datetime format\n",
    "news_dataset.loc[:, 'date'] = pd.to_datetime(news_dataset['date'], format='%Y/%m/%d', errors='coerce')\n",
    "# Use loc to create a new column with the desired format\n",
    "news_dataset.loc[:, 'formatted_date'] = news_dataset['date'].dt.strftime('%Y-%m-%d')\n",
    "# drop the date column\n",
    "news_dataset = news_dataset.drop(columns=['date'])\n",
    "# rename the formatted_date column to date\n",
    "news_dataset = news_dataset.rename(columns={\"formatted_date\": \"date\"})\n",
    "\n",
    "# Remove \"\\n\" from author column\n",
    "news_dataset['author'] = news_dataset['author'].str.replace(\"\\n\", \"\")\n",
    "\n",
    "# add id as \"A\" + id column\n",
    "news_dataset['code'] = \"A\" + news_dataset.index.astype(str)\n",
    "\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows where content starts with \", I want to receive updates from partners and sponsors., , \"\n",
    "news_dataset[news_dataset['content'].str.startswith(\".\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_count_news_dataset = news_dataset.shape[0]\n",
    "print(\"Number of rows: \", rows_count_news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the first 70% of the DataFrame\n",
    "num_rows = int(len(news_dataset) * 0.9)\n",
    "first_70_percent = news_dataset.iloc[:num_rows]\n",
    "news_dataset = news_dataset.iloc[num_rows:] # The remaining 30% of the DataFrame\n",
    "\n",
    "# Filter rows containing both 'Trump' and 'Lebron' in the 'Title' column\n",
    "filtered_data = first_70_percent[\n",
    "    (first_70_percent['title'].str.contains('Trump', case=False)) |\n",
    "    (first_70_percent['title'].str.contains('Lebron', case=False)) |\n",
    "    (first_70_percent['title'].str.contains('FBI', case=False)) |\n",
    "    (first_70_percent['title'].str.contains('gun', case=False))\n",
    "]\n",
    "\n",
    "news_dataset = pd.concat([filtered_data, news_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_count_news_dataset = news_dataset.shape[0]\n",
    "print(\"Number of rows: \", rows_count_news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation in the command line first: pip install clean-text\n",
    "\n",
    "from cleantext.sklearn import CleanTransformer\n",
    "\n",
    "cleaner = CleanTransformer(no_punct=False, lower=False)\n",
    "\n",
    "news_dataset['content'] = cleaner.transform(news_dataset['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation in the command line first: pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_title(model, title):\n",
    "    embedding = model.encode([title])\n",
    "    return [float(w) for w in embedding[0]]\n",
    "\n",
    "def update_dataframe_with_embeddings(df):\n",
    "    try:\n",
    "        model_name = 'all-MiniLM-L6-v2'\n",
    "        model = SentenceTransformer(model_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred initializing model:\", e)\n",
    "        return df\n",
    "\n",
    "    df['vector'] = df['title'].apply(lambda title: embed_title(model, title))\n",
    "    return df\n",
    "\n",
    "update_dataframe_with_embeddings(news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select every column except keyphrases and url\n",
    "news_database = news_dataset[['title', 'author', 'date', 'content', 'publisher', 'source', 'category', 'code', 'vector']]\n",
    "\n",
    "# generate json file to data folder\n",
    "news_database.to_json('./../solr/news.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyphrases Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation in the command line first: pip install rake-nltk\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters by\n",
    "r = Rake()\n",
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(row):\n",
    "    r.extract_keywords_from_text(row['title'] + ' ' + row['content'])\n",
    "    keywords_list = r.get_ranked_phrases()\n",
    "    return ';'.join(keywords_list)\n",
    "\n",
    "# Apply the function to create a new 'keywords' column\n",
    "news_dataset['keyphrases'] = news_dataset.apply(extract_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition and Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation in the command line first: pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", collocations=False).generate(' '.join(news_dataset['keyphrases']))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of 10,000 objects is selected, and from each row, the keyphrases are extracted. From these keyphrases, we will identify named entities (like names of people or places) using spacy. Then, it creates two strings, one containing all the identified entities and another with their corresponding labels. After that, we generate word clouds for both the entities and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a sample of 10000 rows\n",
    "news_content = news_dataset.sample(n=10000)['keyphrases']\n",
    "\n",
    "all_entities = []\n",
    "all_labels = []\n",
    "\n",
    "# collect all entities and labels\n",
    "for text in news_content:\n",
    "    # Apply NER\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    labels = [ent.label_ for ent in doc.ents]\n",
    "    all_labels.extend(labels)\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "entity_string = ' '.join(all_entities)\n",
    "labels_string = ' '.join(all_labels)\n",
    "\n",
    "# create wordclouds\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", collocations=False).generate(entity_string)\n",
    "\n",
    "plt.title('Entities', fontsize=20, fontweight='bold', pad=20, loc='left', color='black', fontfamily='serif', y=1.02)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", collocations=False).generate(labels_string)\n",
    "\n",
    "plt.title('Labels', fontsize=20, fontweight='bold', pad=20, loc='left', color='black', fontfamily='serif', y=1.02)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_analysis = news_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Source distribution\n",
    "source_counts = news_dataset_analysis['source'].value_counts()\n",
    "threshold = 0.05  # Define a threshold for combining into 'Others'\n",
    "small_sources = source_counts[source_counts / source_counts.sum() < threshold].index\n",
    "news_dataset_analysis['source'] = news_dataset_analysis['source'].apply(lambda x: 'Others' if x in small_sources else x)\n",
    "source_counts = news_dataset_analysis['source'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(source_counts, labels=source_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Source Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Publisher distribution\n",
    "publisher_counts = news_dataset_analysis['publisher'].value_counts()\n",
    "small_publishers = publisher_counts[publisher_counts / publisher_counts.sum() < threshold].index\n",
    "news_dataset_analysis['publisher'] = news_dataset_analysis['publisher'].apply(lambda x: 'Others' if x in small_publishers else x)\n",
    "publisher_counts = news_dataset_analysis['publisher'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(publisher_counts, labels=publisher_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Publisher Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Year distribution\n",
    "def extract_year(date_str):\n",
    "    try:\n",
    "        return int(date_str.split('-')[0])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "news_dataset_analysis['year'] = news_dataset_analysis['date'].apply(extract_year)\n",
    "\n",
    "df = news_dataset_analysis.dropna(subset=['year'])\n",
    "year_counts = df['year'].value_counts()\n",
    "\n",
    "threshold = 0.03  # Adjust the threshold as needed\n",
    "small_years = year_counts[year_counts / year_counts.sum() < threshold].index\n",
    "year_counts.loc['Others'] = year_counts[small_years].sum()\n",
    "year_counts.drop(small_years, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(year_counts, labels=year_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Year Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_analysis['article_length'] = news_dataset_analysis['content'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = [form * 500 for form in range(0, 95)]\n",
    "\n",
    "# Create a histogram of article lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(news_dataset_analysis['article_length'], bins=bin_edges, edgecolor='k')\n",
    "plt.xlabel('Article Length (Characters)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Article Length Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of article lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(news_dataset_analysis['article_length'], vert=False)\n",
    "plt.ylabel('Article Length (Characters)')\n",
    "plt.title('Article Length Box Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "news_dataset_analysis['year'] = news_dataset_analysis['date'].apply(extract_year)\n",
    "df = news_dataset_analysis.dropna(subset=['year'])\n",
    "year_counts = df['year'].value_counts()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='year', values='title', aggfunc='count')\n",
    "\n",
    "# Create the heatmap no analyse the number of articles published each year\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.xlabel('Year of Publication')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Publication Year Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux_df = news_dataset_analysis.copy()\n",
    "\n",
    "# Extract the year of publication\n",
    "aux_df['year'] = pd.to_datetime(aux_df['date']).dt.year\n",
    "\n",
    "# Calculate the ratio of content length to publication date\n",
    "aux_df['content_length'] = aux_df['content'].apply(len)\n",
    "aux_df['ratio'] = aux_df['content_length'] / aux_df['year']\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(aux_df['year'], aux_df['ratio'], s=100, alpha=0.7)\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Content Length / Year')\n",
    "plt.title('Scatter Plot of Content Length vs. Publication Year')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 1st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible values referencing trump\n",
    "filter1 = [' trump ', ' donald ']\n",
    "filter2 = ['in-migration', 'immigration']\n",
    "filter3 = ['exile', 'deportation', 'expatriation', 'transportation']\n",
    "filter4 = ['refugee', 'refugees']\n",
    "filter5 = ['migrant', 'migrants']\n",
    "filter6 = ['ban']\n",
    "\n",
    "result = news_dataset[news_dataset['keyphrases'].str.contains('|'.join(filter1))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter2))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter3))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter4))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter5))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter6))]\n",
    "\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 2st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible items to be used for evaluation\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 3st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible items to be used for evaluation\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 4st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible items to be used for evaluation\n",
    "\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 1: Find news articles where Trump spoke on the immigration crisis\n",
    "\n",
    "Query 2: Find news about LeBron's good performances in lost games\n",
    "\n",
    "Query 3: Find articles related to homicides investigated by the FBI in 2017\n",
    "\n",
    "Query 4: Find news articles regarding the conflicts between republicans and democrats about gun ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QRELS1_FILE = \"./../data_evaluation/qrels/query1_qrels.txt\"\n",
    "QRELS2_FILE = \"./../data_evaluation/qrels/query2_qrels.txt\"\n",
    "QRELS3_FILE = \"./../data_evaluation/qrels/query3_qrels.txt\"\n",
    "QRELS4_FILE = \"./../data_evaluation/qrels/query4_qrels.txt\"\n",
    "\n",
    "QUERY1_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=Trump+immigration+crisis&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY1_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^2.5+content&q=Trump+immigration+crisis&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score&bq=title:Trump^2.5 title:immigration^1.5 content:Trump^2.5 content:immigration^1.5'\n",
    "\n",
    "QUERY2_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=LeBron+good+performance+lost+game+points&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY2_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^1.5+content&q=LeBron+good+performance+lost+game+points&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score&bq=title:Lebron^3 title:lost^1.5 title:game^1.5 content:Lebron^2.5 content:lost^2 content:game^1.5'\n",
    "\n",
    "QUERY3_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=homicide+FBI&indent=true&q.op=AND&fq=date:[2017-01-01T00:00:00Z TO 2017-12-31T23:59:59Z]&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY3_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^2+content&q=homicide+FBI&indent=true&q.op=AND&fq=date:[2017-01-01T00:00:00Z TO 2017-12-31T23:59:59Z]&fl=code,title,author,date,publisher,category,content,score&bq=title:homicides^2.0 content:homicides^2.0'\n",
    "\n",
    "QUERY4_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=Republicans+Democrats+\"gun+ownership\"+conflicts&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY4_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^2.5+content&q=Republicans+Democrats+\"gun+ownership\"+conflicts&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score&bq=title:gun^2.5 title:conflicts^1.5 content:gun^2.5 content:conflicts^1.5'\n",
    "\n",
    "# Read qrels to extract relevant documents\n",
    "relevant = list(map(lambda el: el.strip(), open(QRELS1_FILE).readlines())) # Change to QRELS2_FILE, QRELS3_FILE or QRELS4_FILE to see the results for other queries\n",
    "# Get query results from Solr instance\n",
    "results = requests.get(QUERY1_URL_WITHOUT_BOOST).json()['response']['docs'] # Without boost -> change to with boost to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS TABLE\n",
    "# Define custom decorator to automatically calculate metric based on key\n",
    "metrics = {}\n",
    "metric = lambda f: metrics.setdefault(f.__name__, f)\n",
    "\n",
    "@metric\n",
    "def ap(results, relevant):\n",
    "    \"\"\"Average Precision\"\"\"\n",
    "    precision_values = []\n",
    "    relevant_count = 0\n",
    "\n",
    "    for idx, doc in enumerate(results):\n",
    "        if doc['code'] in relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / (idx + 1)\n",
    "            precision_values.append(precision_at_k)\n",
    "\n",
    "    if not precision_values:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precision_values)/len(precision_values)\n",
    "\n",
    "@metric\n",
    "def p10(results, relevant, n=10):\n",
    "    \"\"\"Precision at N\"\"\"\n",
    "    return len([doc for doc in results[:n] if doc['code'] in relevant])/n\n",
    "\n",
    "def calculate_metric(key, results, relevant):\n",
    "    return metrics[key](results, relevant)\n",
    "\n",
    "# Define metrics to be calculated\n",
    "evaluation_metrics = {\n",
    "    'ap': 'Average Precision',\n",
    "    'p10': 'Precision at 10 (P@10)'\n",
    "}\n",
    "\n",
    "# Calculate all metrics and export results as LaTeX table\n",
    "df = pd.DataFrame([['Metric','Value']] +\n",
    "    [\n",
    "        [evaluation_metrics[m], calculate_metric(m, results, relevant)]\n",
    "        for m in evaluation_metrics\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open('results.tex','w') as tf:\n",
    "    tf.write(df.to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision = [\n",
    "    len([\n",
    "        doc \n",
    "        for doc in results[:idx]\n",
    "        if doc['code'] in relevant\n",
    "    ]) / idx \n",
    "    for idx, _ in enumerate(results, start=1)\n",
    "]\n",
    "\n",
    "recall = [\n",
    "    len([\n",
    "        doc for doc in results[:idx]\n",
    "        if doc['code'] in relevant\n",
    "    ]) / len(relevant)\n",
    "    for idx, _ in enumerate(results, start=1)\n",
    "]\n",
    "\n",
    "precision2 = copy.deepcopy(precision)\n",
    "i = len(recall) - 2\n",
    "\n",
    "# interpolation...\n",
    "while i>=0:\n",
    "    if precision[i+1] > precision[i]:\n",
    "        precision[i] = precision[i+1]\n",
    "    i = i - 1\n",
    "\n",
    "# plotting...\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(recall) - 1):\n",
    "    ax.plot((recall[i], recall[i]), (precision[i], precision[i+1]), 'k-' ,label='', color='red') #vertical\n",
    "    ax.plot((recall[i], recall[i+1]), (precision[i+1], precision[i+1]), 'k-', label='', color='red') #horizontal\n",
    "\n",
    "ax.plot(recall,precision2,'k--',color='blue')\n",
    "ax.set_xlabel(\"recall\")\n",
    "ax.set_ylabel(\"precision\")\n",
    "plt.savefig('precision_recall.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
