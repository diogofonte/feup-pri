{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Information Processing and Retrieval**\n",
    "\n",
    "**Project developed by:**\n",
    "- Diogo Fonte - up202004175\n",
    "- Rodrigo Figueiredo - up202005216\n",
    "- Sofia Rodrigo  - up202301429\n",
    "- Vítor Cavaleiro - up202004724\n",
    "\n",
    "## **Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "### All The News - Collection of Articles from 18 publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original file is a .db file, which was exported as a json file using the sqlite studio\n",
    "\n",
    "# get table with rows and columns\n",
    "f = open(\"../data/all-the-news/all-the-news-conv.json\", encoding=\"utf8\")\n",
    "data = json.load(f)\n",
    "table = data[\"objects\"][0]\n",
    "\n",
    "# get rows and columns\n",
    "columns = table[\"columns\"]\n",
    "rows = table[\"rows\"]\n",
    "\n",
    "# get column names\n",
    "column_names = []\n",
    "for column in columns:\n",
    "    column_names.append(column[\"name\"])\n",
    "\n",
    "# Create resulting dictionary\n",
    "result = {}\n",
    "for column_name in column_names:\n",
    "    result[column_name] = []\n",
    "\n",
    "# get rows\n",
    "for row in rows:\n",
    "    for i in range(len(column_names)):\n",
    "        result[column_names[i]].append(row[i])\n",
    "\n",
    "pd.DataFrame.from_dict(result).to_csv('all_the_news.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilizador\\AppData\\Local\\Temp\\ipykernel_16180\\1132100165.py:1: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_the_news = pd.read_csv('all_the_news.csv', encoding='utf-8')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          0\n",
       "id                  0\n",
       "title               3\n",
       "author          32400\n",
       "date            12605\n",
       "content         15403\n",
       "year            12605\n",
       "month           12605\n",
       "publication      7715\n",
       "category        35422\n",
       "digital         11020\n",
       "section        129563\n",
       "url            105339\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_the_news = pd.read_csv('all_the_news.csv', encoding='utf-8')\n",
    "all_the_news.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>category</th>\n",
       "      <th>digital</th>\n",
       "      <th>section</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                              title  \\\n",
       "0           0   1  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1           1   2                                  AI, the humanity!   \n",
       "2           2   3                                  The Viral Machine   \n",
       "3           3   4  How Anker is beating Apple and Samsung at thei...   \n",
       "4           4   5  Tour Black Panther’s reimagined homeland with ...   \n",
       "\n",
       "                author        date  \\\n",
       "0   \\nTasha Robinson\\n  2017-05-31   \n",
       "1       \\nSam Byford\\n  2017-05-30   \n",
       "2  \\nKaitlyn Tiffany\\n  2017-05-25   \n",
       "3       \\nNick Statt\\n  2017-05-22   \n",
       "4       \\nKwame Opam\\n  2017-05-15   \n",
       "\n",
       "                                             content    year  month  \\\n",
       "0        And never more so than in Showtime’s new...  2017.0    5.0   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  2017.0    5.0   \n",
       "2        Super Deluxe built a weird internet empi...  2017.0    5.0   \n",
       "3        Steven Yang quit his job at Google in th...  2017.0    5.0   \n",
       "4        Ahead of Black Panther’s 2018 theatrical...  2017.0    5.0   \n",
       "\n",
       "  publication  category  digital section  url  \n",
       "0       Verge  Longform      1.0     NaN  NaN  \n",
       "1       Verge  Longform      1.0     NaN  NaN  \n",
       "2       Verge  Longform      1.0     NaN  NaN  \n",
       "3       Verge  Longform      1.0     NaN  NaN  \n",
       "4       Verge  Longform      1.0     NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_the_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title               author  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \\nTasha Robinson\\n   \n",
       "1                                  AI, the humanity!       \\nSam Byford\\n   \n",
       "2                                  The Viral Machine  \\nKaitlyn Tiffany\\n   \n",
       "3  How Anker is beating Apple and Samsung at thei...       \\nNick Statt\\n   \n",
       "4  Tour Black Panther’s reimagined homeland with ...       \\nKwame Opam\\n   \n",
       "\n",
       "         date                                            content publisher  \\\n",
       "0  2017-05-31        And never more so than in Showtime’s new...     Verge   \n",
       "1  2017-05-30        AlphaGo’s victory isn’t a defeat for hum...     Verge   \n",
       "2  2017-05-25        Super Deluxe built a weird internet empi...     Verge   \n",
       "3  2017-05-22        Steven Yang quit his job at Google in th...     Verge   \n",
       "4  2017-05-15        Ahead of Black Panther’s 2018 theatrical...     Verge   \n",
       "\n",
       "     source category  url  \n",
       "0  Longform      NaN  NaN  \n",
       "1  Longform      NaN  NaN  \n",
       "2  Longform      NaN  NaN  \n",
       "3  Longform      NaN  NaN  \n",
       "4  Longform      NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops irrelevant columns\n",
    "all_the_news = all_the_news.drop(columns=['Unnamed: 0', 'id', 'year', 'month', 'digital'])\n",
    "all_the_news = all_the_news.rename(columns={\"publication\": \"publisher\"})\n",
    "all_the_news = all_the_news.rename(columns={\"category\": \"source\"})\n",
    "all_the_news = all_the_news.rename(columns={\"section\": \"category\"})\n",
    "all_the_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  204135\n"
     ]
    }
   ],
   "source": [
    "rows_count = all_the_news.shape[0]\n",
    "print(\"Number of rows: \", rows_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Domec...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  author        date  \\\n",
       "0  Ad sales boost Time Warner profit     NaN  2005-12-31   \n",
       "1   Dollar gains on Greenspan speech     NaN  2005-12-31   \n",
       "2  Yukos unit buyer faces loan claim     NaN  2005-12-31   \n",
       "3  High fuel prices hit BA's profits     NaN  2005-12-31   \n",
       "4  Pernod takeover talk lifts Domecq     NaN  2005-12-31   \n",
       "\n",
       "                                             content publisher   source  \\\n",
       "0  Quarterly profits at US media giant TimeWarner...       BBC  website   \n",
       "1  The dollar has hit its highest level against t...       BBC  website   \n",
       "2  The owners of embattled Russian oil giant Yuko...       BBC  website   \n",
       "3  British Airways has blamed high fuel prices fo...       BBC  website   \n",
       "4  Shares in UK drinks and food firm Allied Domec...       BBC  website   \n",
       "\n",
       "   category  url  \n",
       "0  business  NaN  \n",
       "1  business  NaN  \n",
       "2  business  NaN  \n",
       "3  business  NaN  \n",
       "4  business  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_folder = \"../data/bbc_news_collection/\"\n",
    "news = []\n",
    "\n",
    "# Iterate through subfolders of the 5 categories (business, entertainment, politics, sport, tech)\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(subfolder_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                    author = np.nan  # No author information\n",
    "                    date = \"2005-12-31\"\n",
    "                    lines = file.readlines()\n",
    "                    title = lines[0].strip()  # Read the first line as the title\n",
    "                    content = \"\".join(lines[1:]).replace(\"\\n\", \" \").strip()  # Read the rest as content\n",
    "                    publisher = \"BBC\"\n",
    "                    category = subfolder\n",
    "                    url = np.nan\n",
    "                    source = \"website\"\n",
    "\n",
    "                    aux = pd.DataFrame({\"title\": [title], \"author\": [author], \"date\": [date],\n",
    "                                        \"content\": [content], \"publisher\": [publisher],  \"source\": [source],\n",
    "                                        \"category\": [category], \"url\": [url]})\n",
    "                    news.append(aux)\n",
    "\n",
    "bbc_news = pd.concat(news, ignore_index=True)\n",
    "bbc_news.to_csv(\"bbc_articles.csv\", index=False)\n",
    "bbc_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  2962\n"
     ]
    }
   ],
   "source": [
    "rows_count_bbc = bbc_news.shape[0]\n",
    "print(\"Number of rows: \", rows_count_bbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title               author  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   \\nTasha Robinson\\n   \n",
       "1                                  AI, the humanity!       \\nSam Byford\\n   \n",
       "2                                  The Viral Machine  \\nKaitlyn Tiffany\\n   \n",
       "3  How Anker is beating Apple and Samsung at thei...       \\nNick Statt\\n   \n",
       "4  Tour Black Panther’s reimagined homeland with ...       \\nKwame Opam\\n   \n",
       "\n",
       "         date                                            content publisher  \\\n",
       "0  2017-05-31        And never more so than in Showtime’s new...     Verge   \n",
       "1  2017-05-30        AlphaGo’s victory isn’t a defeat for hum...     Verge   \n",
       "2  2017-05-25        Super Deluxe built a weird internet empi...     Verge   \n",
       "3  2017-05-22        Steven Yang quit his job at Google in th...     Verge   \n",
       "4  2017-05-15        Ahead of Black Panther’s 2018 theatrical...     Verge   \n",
       "\n",
       "     source category  url  \n",
       "0  Longform      NaN  NaN  \n",
       "1  Longform      NaN  NaN  \n",
       "2  Longform      NaN  NaN  \n",
       "3  Longform      NaN  NaN  \n",
       "4  Longform      NaN  NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset = pd.concat([all_the_news, bbc_news], ignore_index=True)\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  207097\n"
     ]
    }
   ],
   "source": [
    "rows_count_news_dataset = news_dataset.shape[0]\n",
    "print(\"Number of rows: \", rows_count_news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilizador\\AppData\\Local\\Temp\\ipykernel_16180\\4053499288.py:18: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  news_dataset.loc[:, 'date'] = pd.to_datetime(news_dataset['date'], format='%Y/%m/%d', errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>Tasha Robinson</td>\n",
       "      <td>And never more so than in Showtime’s new serie...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>Sam Byford</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>Kaitlyn Tiffany</td>\n",
       "      <td>Super Deluxe built a weird internet empire. Ca...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>Nick Statt</td>\n",
       "      <td>Steven Yang quit his job at Google in the summ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>Kwame Opam</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical relea...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>A4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   Tasha Robinson   \n",
       "1                                  AI, the humanity!       Sam Byford   \n",
       "2                                  The Viral Machine  Kaitlyn Tiffany   \n",
       "3  How Anker is beating Apple and Samsung at thei...       Nick Statt   \n",
       "4  Tour Black Panther’s reimagined homeland with ...       Kwame Opam   \n",
       "\n",
       "                                             content publisher    source  \\\n",
       "0  And never more so than in Showtime’s new serie...     Verge  Longform   \n",
       "1  AlphaGo’s victory isn’t a defeat for humans — ...     Verge  Longform   \n",
       "2  Super Deluxe built a weird internet empire. Ca...     Verge  Longform   \n",
       "3  Steven Yang quit his job at Google in the summ...     Verge  Longform   \n",
       "4  Ahead of Black Panther’s 2018 theatrical relea...     Verge  Longform   \n",
       "\n",
       "  category  url        date code  \n",
       "0      NaN  NaN  2017-05-31   A0  \n",
       "1      NaN  NaN  2017-05-30   A1  \n",
       "2      NaN  NaN  2017-05-25   A2  \n",
       "3      NaN  NaN  2017-05-22   A3  \n",
       "4      NaN  NaN  2017-05-15   A4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = ['title', 'author', 'content', 'publisher', 'source', 'category', 'url']\n",
    "\n",
    "# strip the text columns\n",
    "news_dataset[text_columns] = news_dataset[text_columns].apply(lambda x: x.str.strip())\n",
    "\n",
    "# Remove duplicates\n",
    "news_dataset = news_dataset.drop_duplicates(subset=['title'], keep='first')\n",
    "news_dataset = news_dataset.drop_duplicates(subset=['content'], keep='first')\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "news_dataset[text_columns] = news_dataset[text_columns].replace('', np.nan)\n",
    "\n",
    "# drop na values from title and content columns\n",
    "news_dataset = news_dataset.dropna(subset=['title'])\n",
    "news_dataset = news_dataset.dropna(subset=['content'])\n",
    "\n",
    "# Convert 'date' column to datetime format\n",
    "news_dataset.loc[:, 'date'] = pd.to_datetime(news_dataset['date'], format='%Y/%m/%d', errors='coerce')\n",
    "# Use loc to create a new column with the desired format\n",
    "news_dataset.loc[:, 'formatted_date'] = news_dataset['date'].dt.strftime('%Y-%m-%d')\n",
    "# drop the date column\n",
    "news_dataset = news_dataset.drop(columns=['date'])\n",
    "# rename the formatted_date column to date\n",
    "news_dataset = news_dataset.rename(columns={\"formatted_date\": \"date\"})\n",
    "\n",
    "# Remove \"\\n\" from author column\n",
    "news_dataset['author'] = news_dataset['author'].str.replace(\"\\n\", \"\")\n",
    "\n",
    "# add id as \"A\" + id column\n",
    "news_dataset['code'] = \"A\" + news_dataset.index.astype(str)\n",
    "\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Google’s quantum computer just flunked its fir...</td>\n",
       "      <td>Russell Brandom</td>\n",
       "      <td>.chorus-snippet p toggles {display:none}When t...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-06-19</td>\n",
       "      <td>A673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>The plot to kill the password</td>\n",
       "      <td>Russell Brandom</td>\n",
       "      <td>.story-image {background-image: url('\"\"&amp;gtnew ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>A750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>The CUPID drone strikes with 80,000 volts to t...</td>\n",
       "      <td>Russell Brandom</td>\n",
       "      <td>.social-col:before {           content: \"\"    ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>A788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>'Mario Kart' in real life is real weird</td>\n",
       "      <td>Dieter Bohn</td>\n",
       "      <td>.social-col:before {           content: \"\"    ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>A789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Google's Eric Schmidt: 'let us celebrate capit...</td>\n",
       "      <td>Adrianne Jeffries</td>\n",
       "      <td>.social-col:before {         content: \"\"      ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>A790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Revenge of the jocks: sports invades tech's Te...</td>\n",
       "      <td>Adrianne Jeffries</td>\n",
       "      <td>.social-col:before {         content: \"\"      ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>A795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>First HomeKit devices confirm Apple TV's limit...</td>\n",
       "      <td>Thomas Ricker</td>\n",
       "      <td>.badge-ces{position:relative top:-103px margin...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Exclusive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>A1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98886</th>\n",
       "      <td>J.R. Smith asks for prayers as daughter is bor...</td>\n",
       "      <td>Laura Italiano</td>\n",
       "      <td>.  and his wife share difficult family news.  ...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170108020204/htt...</td>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>A98886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104165</th>\n",
       "      <td>Why the ‘Passengers’ Lives Matter’ revolt is u...</td>\n",
       "      <td>Nathan Henderson</td>\n",
       "      <td>. I love being a flight attendant, but recentl...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170423021820/htt...</td>\n",
       "      <td>2017-04-22</td>\n",
       "      <td>A104165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107082</th>\n",
       "      <td>Freakishly large lobster boards plane no quest...</td>\n",
       "      <td>Ruth Brown</td>\n",
       "      <td>.  officers are skilled at screening all sorts...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170627012419/htt...</td>\n",
       "      <td>2017-06-26</td>\n",
       "      <td>A107082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108740</th>\n",
       "      <td>A peek at ‘Hamilton’ star Leslie Odom Jr.’s Ch...</td>\n",
       "      <td>Alexis Benveniste</td>\n",
       "      <td>. shined in “ ” and now he’s ready to shine du...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>http://nypost.com/2016/12/16/a-peek-at-hamilto...</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>A108740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109298</th>\n",
       "      <td>Congresswoman who lost her legs in Iraq shuts ...</td>\n",
       "      <td>Jackie Salo</td>\n",
       "      <td>. , this is how one usually looks when you are...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>http://nypost.com/2016/08/02/congresswoman-who...</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>A109298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113154</th>\n",
       "      <td>This guy has been ‘killed’ in several recent t...</td>\n",
       "      <td>Natalie O'Neill</td>\n",
       "      <td>.   my brother traveled there, I'm scared for ...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>http://nypost.com/2016/07/05/this-guy-has-been...</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>A113154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116323</th>\n",
       "      <td>Football player rescues unconscious woman from...</td>\n",
       "      <td>Mark W. Sanchez</td>\n",
       "      <td>.  LB Cristian Garcia helps unconscious woman ...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>http://nypost.com/2016/07/23/football-player-r...</td>\n",
       "      <td>2016-07-23</td>\n",
       "      <td>A116323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154713</th>\n",
       "      <td>“Extreme vetting” is already the reality for r...</td>\n",
       "      <td>Robyn Jordan</td>\n",
       "      <td>.c-entry-content .e-image {\\nwidth: 30%;\\ndisp...</td>\n",
       "      <td>Vox</td>\n",
       "      <td>general</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.vox.com/first-person/2017/4/4/15160...</td>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>A154713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169984</th>\n",
       "      <td>Kellyanne Conway: Mitt Romney ‘went out of his...</td>\n",
       "      <td>Amy B Wang</td>\n",
       "      <td>.@KellyannePolls on Mitt Romney: \"I don't thin...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>newspaper</td>\n",
       "      <td>news</td>\n",
       "      <td>https://web.archive.org/web/20161128003044/htt...</td>\n",
       "      <td>2016-11-27</td>\n",
       "      <td>A169984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title             author  \\\n",
       "673     Google’s quantum computer just flunked its fir...    Russell Brandom   \n",
       "750                         The plot to kill the password    Russell Brandom   \n",
       "788     The CUPID drone strikes with 80,000 volts to t...    Russell Brandom   \n",
       "789               'Mario Kart' in real life is real weird        Dieter Bohn   \n",
       "790     Google's Eric Schmidt: 'let us celebrate capit...  Adrianne Jeffries   \n",
       "795     Revenge of the jocks: sports invades tech's Te...  Adrianne Jeffries   \n",
       "1073    First HomeKit devices confirm Apple TV's limit...      Thomas Ricker   \n",
       "98886   J.R. Smith asks for prayers as daughter is bor...     Laura Italiano   \n",
       "104165  Why the ‘Passengers’ Lives Matter’ revolt is u...   Nathan Henderson   \n",
       "107082  Freakishly large lobster boards plane no quest...         Ruth Brown   \n",
       "108740  A peek at ‘Hamilton’ star Leslie Odom Jr.’s Ch...  Alexis Benveniste   \n",
       "109298  Congresswoman who lost her legs in Iraq shuts ...        Jackie Salo   \n",
       "113154  This guy has been ‘killed’ in several recent t...    Natalie O'Neill   \n",
       "116323  Football player rescues unconscious woman from...    Mark W. Sanchez   \n",
       "154713  “Extreme vetting” is already the reality for r...       Robyn Jordan   \n",
       "169984  Kellyanne Conway: Mitt Romney ‘went out of his...         Amy B Wang   \n",
       "\n",
       "                                                  content        publisher  \\\n",
       "673     .chorus-snippet p toggles {display:none}When t...            Verge   \n",
       "750     .story-image {background-image: url('\"\"&gtnew ...            Verge   \n",
       "788     .social-col:before {           content: \"\"    ...            Verge   \n",
       "789     .social-col:before {           content: \"\"    ...            Verge   \n",
       "790     .social-col:before {         content: \"\"      ...            Verge   \n",
       "795     .social-col:before {         content: \"\"      ...            Verge   \n",
       "1073    .badge-ces{position:relative top:-103px margin...            Verge   \n",
       "98886   .  and his wife share difficult family news.  ...    New York Post   \n",
       "104165  . I love being a flight attendant, but recentl...    New York Post   \n",
       "107082  .  officers are skilled at screening all sorts...    New York Post   \n",
       "108740  . shined in “ ” and now he’s ready to shine du...    New York Post   \n",
       "109298  . , this is how one usually looks when you are...    New York Post   \n",
       "113154  .   my brother traveled there, I'm scared for ...    New York Post   \n",
       "116323  .  LB Cristian Garcia helps unconscious woman ...    New York Post   \n",
       "154713  .c-entry-content .e-image {\\nwidth: 30%;\\ndisp...              Vox   \n",
       "169984  .@KellyannePolls on Mitt Romney: \"I don't thin...  Washington Post   \n",
       "\n",
       "           source category                                                url  \\\n",
       "673       Reports      NaN                                                NaN   \n",
       "750       Reports      NaN                                                NaN   \n",
       "788       Reports      NaN                                                NaN   \n",
       "789       Reports      NaN                                                NaN   \n",
       "790       Reports      NaN                                                NaN   \n",
       "795       Reports      NaN                                                NaN   \n",
       "1073    Exclusive      NaN                                                NaN   \n",
       "98886         NaN     2017  https://web.archive.org/web/20170108020204/htt...   \n",
       "104165        NaN     2017  https://web.archive.org/web/20170423021820/htt...   \n",
       "107082        NaN     2017  https://web.archive.org/web/20170627012419/htt...   \n",
       "108740        NaN     2016  http://nypost.com/2016/12/16/a-peek-at-hamilto...   \n",
       "109298        NaN     2016  http://nypost.com/2016/08/02/congresswoman-who...   \n",
       "113154        NaN     2016  http://nypost.com/2016/07/05/this-guy-has-been...   \n",
       "116323        NaN     2016  http://nypost.com/2016/07/23/football-player-r...   \n",
       "154713    general      NaN  http://www.vox.com/first-person/2017/4/4/15160...   \n",
       "169984  newspaper     news  https://web.archive.org/web/20161128003044/htt...   \n",
       "\n",
       "              date     code  \n",
       "673     2014-06-19     A673  \n",
       "750     2014-04-15     A750  \n",
       "788     2014-03-08     A788  \n",
       "789     2014-03-08     A789  \n",
       "790     2014-03-07     A790  \n",
       "795     2014-03-07     A795  \n",
       "1073    2015-01-08    A1073  \n",
       "98886   2017-01-07   A98886  \n",
       "104165  2017-04-22  A104165  \n",
       "107082  2017-06-26  A107082  \n",
       "108740  2016-12-16  A108740  \n",
       "109298  2016-08-02  A109298  \n",
       "113154  2016-07-05  A113154  \n",
       "116323  2016-07-23  A116323  \n",
       "154713  2017-04-04  A154713  \n",
       "169984  2016-11-27  A169984  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show rows where content starts with \", I want to receive updates from partners and sponsors., , \"\n",
    "news_dataset[news_dataset['content'].str.startswith(\".\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  178530\n"
     ]
    }
   ],
   "source": [
    "rows_count_news_dataset = news_dataset.shape[0]\n",
    "print(\"Number of rows: \", rows_count_news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6bf0bbd2524b258de25e3bc046bc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a5aaacdacd40d2b908a368831dc3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe2ab8b3a5c410f865c0ae665d9e06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a64e70bdd24bdfb74985349b967036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46df7fb6d5f413dbf0e3a7166eb03be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b996875e47c449a9bf82009129ed1e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a81d271b554bc6b6ca13d271867064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eb055cc38d45039d378fbee5a7aec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8963a73f9492993045076a5e76dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc29481a1bd8465aa9f72b5c4298a15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a497fcaf404f12a723bb1c94e0b610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca437c6d69b247bbab3d51b924174c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74027c610afc4bb0a3b2ed43d7d682df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8147cb5eb88347f1b3329487924ca595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>Tasha Robinson</td>\n",
       "      <td>And never more so than in Showtime’s new serie...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>A0</td>\n",
       "      <td>[-0.038446057587862015, -0.03019382245838642, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>Sam Byford</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for humans — ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>A1</td>\n",
       "      <td>[-0.03777475655078888, 0.01548997312784195, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>Kaitlyn Tiffany</td>\n",
       "      <td>Super Deluxe built a weird internet empire. Ca...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>A2</td>\n",
       "      <td>[-0.10532260686159134, -0.007290708366781473, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>Nick Statt</td>\n",
       "      <td>Steven Yang quit his job at Google in the summ...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>A3</td>\n",
       "      <td>[-0.0666596069931984, 0.06303895264863968, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>Kwame Opam</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical relea...</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>A4</td>\n",
       "      <td>[-0.03082909993827343, 0.10280898958444595, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207091</th>\n",
       "      <td>New consoles promise big problems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Making games for future consoles will require ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>tech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>A207091</td>\n",
       "      <td>[0.04849948361515999, -0.07398629188537598, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207092</th>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BT is introducing two initiatives to help beat...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>tech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>A207092</td>\n",
       "      <td>[-0.1111484095454216, -0.021083878353238106, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207094</th>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A new European directive could put software wr...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>tech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>A207094</td>\n",
       "      <td>[-0.04373398423194885, 0.02386585995554924, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207095</th>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The man making sure US computer networks are s...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>tech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>A207095</td>\n",
       "      <td>[0.009242253378033638, -0.004171409178525209, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207096</th>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Online role playing games are time-consuming, ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>website</td>\n",
       "      <td>tech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>A207096</td>\n",
       "      <td>[0.041786059737205505, -0.03337341174483299, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178530 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title           author  \\\n",
       "0       Agent Cooper in Twin Peaks is the audience: on...   Tasha Robinson   \n",
       "1                                       AI, the humanity!       Sam Byford   \n",
       "2                                       The Viral Machine  Kaitlyn Tiffany   \n",
       "3       How Anker is beating Apple and Samsung at thei...       Nick Statt   \n",
       "4       Tour Black Panther’s reimagined homeland with ...       Kwame Opam   \n",
       "...                                                   ...              ...   \n",
       "207091                  New consoles promise big problems              NaN   \n",
       "207092                   BT program to beat dialler scams              NaN   \n",
       "207094                            Be careful how you code              NaN   \n",
       "207095                    US cyber security chief resigns              NaN   \n",
       "207096                   Losing yourself in online gaming              NaN   \n",
       "\n",
       "                                                  content publisher    source  \\\n",
       "0       And never more so than in Showtime’s new serie...     Verge  Longform   \n",
       "1       AlphaGo’s victory isn’t a defeat for humans — ...     Verge  Longform   \n",
       "2       Super Deluxe built a weird internet empire. Ca...     Verge  Longform   \n",
       "3       Steven Yang quit his job at Google in the summ...     Verge  Longform   \n",
       "4       Ahead of Black Panther’s 2018 theatrical relea...     Verge  Longform   \n",
       "...                                                   ...       ...       ...   \n",
       "207091  Making games for future consoles will require ...       BBC   website   \n",
       "207092  BT is introducing two initiatives to help beat...       BBC   website   \n",
       "207094  A new European directive could put software wr...       BBC   website   \n",
       "207095  The man making sure US computer networks are s...       BBC   website   \n",
       "207096  Online role playing games are time-consuming, ...       BBC   website   \n",
       "\n",
       "       category  url        date     code  \\\n",
       "0           NaN  NaN  2017-05-31       A0   \n",
       "1           NaN  NaN  2017-05-30       A1   \n",
       "2           NaN  NaN  2017-05-25       A2   \n",
       "3           NaN  NaN  2017-05-22       A3   \n",
       "4           NaN  NaN  2017-05-15       A4   \n",
       "...         ...  ...         ...      ...   \n",
       "207091     tech  NaN  2005-12-31  A207091   \n",
       "207092     tech  NaN  2005-12-31  A207092   \n",
       "207094     tech  NaN  2005-12-31  A207094   \n",
       "207095     tech  NaN  2005-12-31  A207095   \n",
       "207096     tech  NaN  2005-12-31  A207096   \n",
       "\n",
       "                                                   vector  \n",
       "0       [-0.038446057587862015, -0.03019382245838642, ...  \n",
       "1       [-0.03777475655078888, 0.01548997312784195, -0...  \n",
       "2       [-0.10532260686159134, -0.007290708366781473, ...  \n",
       "3       [-0.0666596069931984, 0.06303895264863968, 0.0...  \n",
       "4       [-0.03082909993827343, 0.10280898958444595, 0....  \n",
       "...                                                   ...  \n",
       "207091  [0.04849948361515999, -0.07398629188537598, 0....  \n",
       "207092  [-0.1111484095454216, -0.021083878353238106, 0...  \n",
       "207094  [-0.04373398423194885, 0.02386585995554924, -0...  \n",
       "207095  [0.009242253378033638, -0.004171409178525209, ...  \n",
       "207096  [0.041786059737205505, -0.03337341174483299, -...  \n",
       "\n",
       "[178530 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_title(model, title):\n",
    "    embedding = model.encode([title])\n",
    "    return [float(w) for w in embedding[0]]\n",
    "\n",
    "def update_dataframe_with_embeddings(df):\n",
    "    try:\n",
    "        model_name = 'all-MiniLM-L6-v2'\n",
    "        model = SentenceTransformer(model_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred initializing model:\", e)\n",
    "        return df\n",
    "\n",
    "    df['vector'] = df['title'].apply(lambda title: embed_title(model, title))\n",
    "    return df\n",
    "\n",
    "update_dataframe_with_embeddings(news_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select every column except keyphrases and url\n",
    "news_database = news_dataset[['title', 'author', 'date', 'content', 'publisher', 'source', 'category', 'code', 'vector']]\n",
    "\n",
    "# generate json file to data folder\n",
    "news_database.to_json('./../solr/news.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyphrases Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation in the command line first: pip install rake-nltk\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters by\n",
    "r = Rake()\n",
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(row):\n",
    "    r.extract_keywords_from_text(row['title'] + ' ' + row['content'])\n",
    "    keywords_list = r.get_ranked_phrases()\n",
    "    return ';'.join(keywords_list)\n",
    "\n",
    "# Apply the function to create a new 'keywords' column\n",
    "news_dataset['keyphrases'] = news_dataset.apply(extract_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition and Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation in the command line first: pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", collocations=False).generate(' '.join(news_dataset['keyphrases']))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of 10,000 objects is selected, and from each row, the keyphrases are extracted. From these keyphrases, we will identify named entities (like names of people or places) using spacy. Then, it creates two strings, one containing all the identified entities and another with their corresponding labels. After that, we generate word clouds for both the entities and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a sample of 10000 rows\n",
    "news_content = news_dataset.sample(n=10000)['keyphrases']\n",
    "\n",
    "all_entities = []\n",
    "all_labels = []\n",
    "\n",
    "# collect all entities and labels\n",
    "for text in news_content:\n",
    "    # Apply NER\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    labels = [ent.label_ for ent in doc.ents]\n",
    "    all_labels.extend(labels)\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "entity_string = ' '.join(all_entities)\n",
    "labels_string = ' '.join(all_labels)\n",
    "\n",
    "# create wordclouds\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", collocations=False).generate(entity_string)\n",
    "\n",
    "plt.title('Entities', fontsize=20, fontweight='bold', pad=20, loc='left', color='black', fontfamily='serif', y=1.02)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", collocations=False).generate(labels_string)\n",
    "\n",
    "plt.title('Labels', fontsize=20, fontweight='bold', pad=20, loc='left', color='black', fontfamily='serif', y=1.02)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_analysis = news_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Source distribution\n",
    "source_counts = news_dataset_analysis['source'].value_counts()\n",
    "threshold = 0.05  # Define a threshold for combining into 'Others'\n",
    "small_sources = source_counts[source_counts / source_counts.sum() < threshold].index\n",
    "news_dataset_analysis['source'] = news_dataset_analysis['source'].apply(lambda x: 'Others' if x in small_sources else x)\n",
    "source_counts = news_dataset_analysis['source'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(source_counts, labels=source_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Source Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Publisher distribution\n",
    "publisher_counts = news_dataset_analysis['publisher'].value_counts()\n",
    "small_publishers = publisher_counts[publisher_counts / publisher_counts.sum() < threshold].index\n",
    "news_dataset_analysis['publisher'] = news_dataset_analysis['publisher'].apply(lambda x: 'Others' if x in small_publishers else x)\n",
    "publisher_counts = news_dataset_analysis['publisher'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(publisher_counts, labels=publisher_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Publisher Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for Year distribution\n",
    "def extract_year(date_str):\n",
    "    try:\n",
    "        return int(date_str.split('-')[0])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "news_dataset_analysis['year'] = news_dataset_analysis['date'].apply(extract_year)\n",
    "\n",
    "df = news_dataset_analysis.dropna(subset=['year'])\n",
    "year_counts = df['year'].value_counts()\n",
    "\n",
    "threshold = 0.03  # Adjust the threshold as needed\n",
    "small_years = year_counts[year_counts / year_counts.sum() < threshold].index\n",
    "year_counts.loc['Others'] = year_counts[small_years].sum()\n",
    "year_counts.drop(small_years, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(year_counts, labels=year_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Year Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_analysis['article_length'] = news_dataset_analysis['content'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = [form * 500 for form in range(0, 95)]\n",
    "\n",
    "# Create a histogram of article lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(news_dataset_analysis['article_length'], bins=bin_edges, edgecolor='k')\n",
    "plt.xlabel('Article Length (Characters)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Article Length Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of article lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(news_dataset_analysis['article_length'], vert=False)\n",
    "plt.ylabel('Article Length (Characters)')\n",
    "plt.title('Article Length Box Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "news_dataset_analysis['year'] = news_dataset_analysis['date'].apply(extract_year)\n",
    "df = news_dataset_analysis.dropna(subset=['year'])\n",
    "year_counts = df['year'].value_counts()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='year', values='title', aggfunc='count')\n",
    "\n",
    "# Create the heatmap no analyse the number of articles published each year\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.xlabel('Year of Publication')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Publication Year Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux_df = news_dataset_analysis.copy()\n",
    "\n",
    "# Extract the year of publication\n",
    "aux_df['year'] = pd.to_datetime(aux_df['date']).dt.year\n",
    "\n",
    "# Calculate the ratio of content length to publication date\n",
    "aux_df['content_length'] = aux_df['content'].apply(len)\n",
    "aux_df['ratio'] = aux_df['content_length'] / aux_df['year']\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(aux_df['year'], aux_df['ratio'], s=100, alpha=0.7)\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Content Length / Year')\n",
    "plt.title('Scatter Plot of Content Length vs. Publication Year')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 1st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible values referencing trump\n",
    "filter1 = [' trump ', ' donald ']\n",
    "filter2 = ['in-migration', 'immigration']\n",
    "filter3 = ['exile', 'deportation', 'expatriation', 'transportation']\n",
    "filter4 = ['refugee', 'refugees']\n",
    "filter5 = ['migrant', 'migrants']\n",
    "filter6 = ['ban']\n",
    "\n",
    "result = news_dataset[news_dataset['keyphrases'].str.contains('|'.join(filter1))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter2))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter3))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter4))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter5))]\n",
    "result = result[result['keyphrases'].str.contains('|'.join(filter6))]\n",
    "\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 2st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible items to be used for evaluation\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 3st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible items to be used for evaluation\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building qrels file - 4st information need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible items to be used for evaluation\n",
    "\n",
    "\n",
    "# print the number of rows\n",
    "rows_count = result.shape[0]\n",
    "print(\"Number of rows: \", rows_count)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "\n",
    "for row in result.iterrows():\n",
    "    title = row[1]['title']\n",
    "    id = row[1]['code']\n",
    "    print(id)\n",
    "    print(title)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 1: Find news articles where Trump spoke on the immigration crisis\n",
    "\n",
    "Query 2: Find news about LeBron's good performances in lost games\n",
    "\n",
    "Query 3: Find articles related to homicides investigated by the FBI in 2017\n",
    "\n",
    "Query 4: Find news articles regarding the conflicts between republicans and democrats about gun ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QRELS1_FILE = \"./../data_evaluation/qrels/query1_qrels.txt\"\n",
    "QRELS2_FILE = \"./../data_evaluation/qrels/query2_qrels.txt\"\n",
    "QRELS3_FILE = \"./../data_evaluation/qrels/query3_qrels.txt\"\n",
    "QRELS4_FILE = \"./../data_evaluation/qrels/query4_qrels.txt\"\n",
    "\n",
    "QUERY1_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=Trump+immigration+crisis&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY1_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^2.5+content&q=Trump+immigration+crisis&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score&bq=title:Trump^2.5 title:immigration^1.5 content:Trump^2.5 content:immigration^1.5'\n",
    "\n",
    "QUERY2_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=LeBron+good+performance+lost+game+points&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY2_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^1.5+content&q=LeBron+good+performance+lost+game+points&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score&bq=title:Lebron^3 title:lost^1.5 title:game^1.5 content:Lebron^2.5 content:lost^2 content:game^1.5'\n",
    "\n",
    "QUERY3_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=homicide+FBI&indent=true&q.op=AND&fq=date:[2017-01-01T00:00:00Z TO 2017-12-31T23:59:59Z]&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY3_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^2+content&q=homicide+FBI&indent=true&q.op=AND&fq=date:[2017-01-01T00:00:00Z TO 2017-12-31T23:59:59Z]&fl=code,title,author,date,publisher,category,content,score&bq=title:homicides^2.0 content:homicides^2.0'\n",
    "\n",
    "QUERY4_URL_WITHOUT_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title+content&q=Republicans+Democrats+\"gun+ownership\"+conflicts&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score'\n",
    "QUERY4_URL_WITH_BOOST = 'http://localhost:8983/solr/news/select?defType=edismax&qf=title^2.5+content&q=Republicans+Democrats+\"gun+ownership\"+conflicts&indent=true&q.op=AND&fl=code,title,author,date,publisher,category,content,score&bq=title:gun^2.5 title:conflicts^1.5 content:gun^2.5 content:conflicts^1.5'\n",
    "\n",
    "# Read qrels to extract relevant documents\n",
    "relevant = list(map(lambda el: el.strip(), open(QRELS1_FILE).readlines())) # Change to QRELS2_FILE, QRELS3_FILE or QRELS4_FILE to see the results for other queries\n",
    "# Get query results from Solr instance\n",
    "results = requests.get(QUERY1_URL_WITHOUT_BOOST).json()['response']['docs'] # Without boost -> change to with boost to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS TABLE\n",
    "# Define custom decorator to automatically calculate metric based on key\n",
    "metrics = {}\n",
    "metric = lambda f: metrics.setdefault(f.__name__, f)\n",
    "\n",
    "@metric\n",
    "def ap(results, relevant):\n",
    "    \"\"\"Average Precision\"\"\"\n",
    "    precision_values = []\n",
    "    relevant_count = 0\n",
    "\n",
    "    for idx, doc in enumerate(results):\n",
    "        if doc['code'] in relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / (idx + 1)\n",
    "            precision_values.append(precision_at_k)\n",
    "\n",
    "    if not precision_values:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precision_values)/len(precision_values)\n",
    "\n",
    "@metric\n",
    "def p10(results, relevant, n=10):\n",
    "    \"\"\"Precision at N\"\"\"\n",
    "    return len([doc for doc in results[:n] if doc['code'] in relevant])/n\n",
    "\n",
    "def calculate_metric(key, results, relevant):\n",
    "    return metrics[key](results, relevant)\n",
    "\n",
    "# Define metrics to be calculated\n",
    "evaluation_metrics = {\n",
    "    'ap': 'Average Precision',\n",
    "    'p10': 'Precision at 10 (P@10)'\n",
    "}\n",
    "\n",
    "# Calculate all metrics and export results as LaTeX table\n",
    "df = pd.DataFrame([['Metric','Value']] +\n",
    "    [\n",
    "        [evaluation_metrics[m], calculate_metric(m, results, relevant)]\n",
    "        for m in evaluation_metrics\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open('results.tex','w') as tf:\n",
    "    tf.write(df.to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = [\n",
    "    {'code': 'A'},\n",
    "    {'code': 'B'},\n",
    "    {'code': 'C'},\n",
    "    {'code': 'D'},\n",
    "    {'code': 'E'},\n",
    "    {'code': 'F'},\n",
    "    {'code': 'G'},\n",
    "    {'code': 'H'},\n",
    "    {'code': 'I'},\n",
    "    {'code': 'J'}\n",
    "]\n",
    "\n",
    "relevant = ['A', 'C', 'D', 'F', 'G', 'I']\n",
    "\n",
    "precision = [\n",
    "    len([\n",
    "        doc \n",
    "        for doc in results[:idx]\n",
    "        if doc['code'] in relevant\n",
    "    ]) / idx \n",
    "    for idx, _ in enumerate(results, start=1)\n",
    "]\n",
    "\n",
    "recall = [\n",
    "    len([\n",
    "        doc for doc in results[:idx]\n",
    "        if doc['code'] in relevant\n",
    "    ]) / len(relevant)\n",
    "    for idx, _ in enumerate(results, start=1)\n",
    "]\n",
    "\n",
    "precision2 = copy.deepcopy(precision)\n",
    "i = len(recall) - 2\n",
    "\n",
    "# interpolation...\n",
    "while i>=0:\n",
    "    if precision[i+1] > precision[i]:\n",
    "        precision[i] = precision[i+1]\n",
    "    i = i - 1\n",
    "\n",
    "# plotting...\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(recall) - 1):\n",
    "    ax.plot((recall[i], recall[i]), (precision[i], precision[i+1]), 'k-' ,label='', color='red') #vertical\n",
    "    ax.plot((recall[i], recall[i+1]), (precision[i+1], precision[i+1]), 'k-', label='', color='red') #horizontal\n",
    "\n",
    "ax.plot(recall,precision2,'k--',color='blue')\n",
    "ax.set_xlabel(\"recall\")\n",
    "ax.set_ylabel(\"precision\")\n",
    "plt.savefig('precision_recall.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
